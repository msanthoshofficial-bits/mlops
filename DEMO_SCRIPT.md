# ðŸŽ¬ Demo Video Script â€” MLOps Assignment 2

> **Duration**: < 5 minutes | **Tool**: OBS Studio or Win+G

## Pre-Recording Setup

- [ ] VS Code open with project
- [ ] MLflow UI running (`python -m mlflow ui` â†’ localhost:5000)
- [ ] API server running (`python -m uvicorn app.main:app --reload`)
- [ ] Browser tabs: MLflow, Swagger (localhost:8000/docs), GitHub Actions
- [ ] Sample cat/dog image on Desktop

---

## â±ï¸ 0:00â€“0:30 â€” Introduction (30s)

**[Show: VS Code project sidebar]**

> *"Hi, I'm Santhosh. This is my end-to-end MLOps pipeline for Cats vs Dogs classification using TensorFlow MobileNetV2, FastAPI, Docker, and GitHub Actions."*

**Show briefly**: `src/`, `app/`, `.github/workflows/`, `Dockerfile`, `docker-compose.yml`

---

## â±ï¸ 0:30â€“1:00 â€” M1: Versioning (30s)

**[Terminal]**

```bash
git log --oneline -5
git lfs ls-files
cat .gitattributes
```

> *"Git tracks source code. Git LFS tracks the model.h5 file which is over 15MB."*

---

## â±ï¸ 1:00â€“2:00 â€” M1: Training & MLflow (60s)

> *"The model was trained using MobileNetV2 transfer learning. All metrics are tracked in MLflow."*

**[Browser â†’ localhost:5000]**

1. Show experiment runs table
2. Click latest run â†’ **Metrics tab** â†’ show `train_loss`, `val_accuracy` charts
3. **Artifacts tab** â†’ click `charts/confusion_matrix.png`, `charts/loss_curve.png`

> *"MLflow logs per-epoch metrics with interactive charts, plus the confusion matrix and classification report as artifacts."*

---

## â±ï¸ 2:00â€“2:45 â€” M2: API & Prediction (45s)

> *"The model is served via FastAPI with a health check and prediction endpoint."*

**[Browser â†’ localhost:8000/docs]**

1. Click **POST /predict** â†’ Try it out
2. Upload sample image â†’ Execute
3. Show JSON response

> *"It correctly predicts [Cat/Dog] with [X]% confidence. The API also exposes Prometheus metrics at /metrics."*

---

## â±ï¸ 2:45â€“3:30 â€” M3: CI Pipeline (45s)

**[VS Code â†’ `.github/workflows/pipeline.yml`]** (briefly)

> *"GitHub Actions runs on every push â€” checkout with LFS, install deps, run pytest, build Docker image, and push to Docker Hub."*

**[Browser â†’ GitHub â†’ Actions tab]**

1. Click latest successful run
2. Expand test step â†’ show pytest passing
3. Show Docker build/push step

> *"The image is published as `msanthoshofficial/cat-dog-classifier:latest`."*

---

## â±ï¸ 3:30â€“4:15 â€” M4: Docker Deployment (45s)

**[Terminal]**

```bash
docker pull msanthoshofficial/cat-dog-classifier:latest
docker run -p 8000:8000 msanthoshofficial/cat-dog-classifier:latest
```

> *"The production image is pulled from Docker Hub and deployed. Let me run the smoke test to verify."*

```bash
python smoke_test.py
```

> *"Health check and prediction both pass â€” deployment is successful."*

---

## â±ï¸ 4:15â€“4:50 â€” M5: Monitoring (35s)

**[Browser â†’ localhost:8000/metrics]**

> *"Prometheus metrics track request count, latency, and status codes for every API call."*

Scroll to show `http_requests_total`, `http_request_duration_seconds`

> *"These can be scraped by a Prometheus server for dashboards and alerting."*

---

## â±ï¸ 4:50â€“5:00 â€” Wrap Up (10s)

> *"This covers the full MLOps lifecycle â€” training, experiment tracking, containerized serving, automated CI/CD, and monitoring. Thank you!"*

---

## Quick Commands Reference

```bash
git log --oneline -5              # Versioning
git lfs ls-files                  # LFS tracking
python -m src.train               # Train model
python -m mlflow ui               # MLflow â†’ localhost:5000
python -m uvicorn app.main:app --reload  # API â†’ localhost:8000
docker pull msanthoshofficial/cat-dog-classifier:latest
docker run -p 8000:8000 msanthoshofficial/cat-dog-classifier:latest
python smoke_test.py              # Smoke test
```
